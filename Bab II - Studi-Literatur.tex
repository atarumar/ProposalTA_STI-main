% ==========================================
% BAB II STUDI LITERATUR
% ==========================================
\chapter{STUDI LITERATUR}
\label{chap:studi-literatur}
%II.1 Transparansi Parlemen=============================================
\section{Transparansi Parlemen}
Transparansi parlemen didefinisikan sebagai prinsip tata kelola yang mewajibkan lembaga legislatif untuk membuka proses pengambilan keputusan, prosedur internal, dan hasil kinerja mereka kepada publik agar dapat dipantau dan dievaluasi secara objektif. Secara filosofis, transparansi ini berakar pada konsep kontrak sosial, di mana warga negara memberikan mandat kekuasaan kepada wakil rakyat dengan syarat adanya akuntabilitas atas penggunaan kekuasaan tersebut.

Berdasarkan analisis yang dilakukan \textcite{ferry2024democracy} mengenai akuntabilitas demokrasi, transparansi parlemen bukan sekadar tujuan akhir, melainkan prasyarat mutlak \textit{sine qua non} bagi terciptanya kepercayaan publik \textit{political trust} \autocite{ferry2024democracy}. Tanpa keterbukaan yang memadai, hubungan antara konstituen dan wakilnya akan mengalami asimetri informasi yang mengarah pada delegitimasi institusi perwakilan. Dalam praktiknya, transparansi ini mencakup tiga dimensi utama, yaitu transparansi prosedur (bagaimana hukum dibuat), transparansi kinerja (apa yang dihasilkan), dan transparansi perilaku (bagaimana integritas aktor politiknya), yang semuanya harus dapat diakses oleh publik untuk menjamin fungsi pengawasan eksternal berjalan efektif.

\subsection{Transparansi Prosedur}
Transparansi prosedur berkaitan dengan keterbukaan mekanisme dan aturan yang mengatur bagaimana keputusan legislatif diambil di dalam parlemen. Dimensi ini menekankan pentingnya akses publik terhadap informasi mengenai tahapan pembentukan undang undang, jadwal persidangan, agenda rapat komisi, serta tata tertib persidangan.

Berdasarkan studi \textcite{bordignon2020rules} mengenai aturan dan akuntabilitas, transparansi prosedur berfungsi sebagai mekanisme pencegah manipulasi agenda \textit{agenda setting manipulation}, di mana kejelasan aturan memastikan bahwa tidak ada keputusan strategis yang diambil secara tersembunyi atau melalui prosedur yang menyimpang \autocite{bordignon2020rules}. Dalam konteks ini, publik harus dapat memverifikasi apakah wakil rakyat telah mematuhi tahapan legislasi yang baku, mulai dari pengusulan naskah akademik, pembahasan tingkat pertama, hingga pengambilan keputusan di rapat paripurna, sehingga legitimasi produk hukum yang dihasilkan dapat dipertanggungjawabkan secara formal.

\subsection{Transparansi Kinerja}
Transparansi kinerja berfokus pada keterbukaan hasil nyata \textit{output} dan dampak \textit{outcome} dari aktivitas legislatif yang dilakukan oleh anggota parlemen, baik secara individu maupun kelembagaan. Dimensi ini mencakup penyediaan data mengenai produk legislasi yang disahkan, partisipasi dalam pemungutan suara \textit{voting records}, kehadiran dalam rapat rapat yang penting, serta kualitas intervensi substantif dalam perdebatan kebijakan.

Mengacu pada penelitian \textcite{chen2023determinants}, transparansi kinerja merupakan indikator utama yang digunakan publik untuk menilai efektivitas wakil rakyat; ketersediaan data kinerja yang terukur memungkinkan konstituen untuk membandingkan antara janji politik yang disampaikan saat kampanye dengan realisasi tindakan di parlemen \autocite{chen2023determinants}. Tanpa transparansi pada dimensi ini, evaluasi publik hanya akan didasarkan pada persepsi atau popularitas semata, bukan pada bukti produktivitas kerja yang objektif, sehingga melemahkan mekanisme penghargaan dan sanksi \textit{reward and punishment mechanism} dalam sistem demokrasi.

\subsection{Transparansi Perilaku}
Transparansi perilaku menyoroti aspek integritas etis dan kepatuhan moral anggota parlemen dalam menjalankan mandat publiknya. Dimensi ini mensyaratkan keterbukaan informasi terkait profil keuangan, deklarasi konflik kepentingan, sumber pendanaan kampanye, serta catatan pelanggaran kode etik yang mungkin dilakukan oleh legislator.

Berdasarkan analisis \textcite{rienks2023corruption} mengenai skandal dan kompetensi politik, transparansi perilaku berfungsi sebagai sistem peringatan dini \textit{early warning system} untuk mendeteksi potensi penyalahgunaan kekuasaan \textit{abuse of power} atau korupsi politik \autocite{rienks2023corruption}. Dengan membuka data perilaku ini, publik dapat memantau apakah keputusan yang diambil oleh seorang legislator didasari oleh kepentingan konstituen atau dipengaruhi oleh kepentingan pribadi dan kelompok tertentu, sehingga menjaga martabat institusi perwakilan dari degradasi moral dan krisis kepercayaan.

%II.2 Transparansi Parlemen Digital=============================================
\section{Transparansi Parlemen Digital}
Transparansi parlemen digital merupakan evolusi dari prinsip keterbukaan tradisional yang didorong oleh adopsi teknologi informasi dan komunikasi dalam lingkungan legislatif. Konsep ini tidak hanya mengubah media penyampaian informasi dari analog ke digital, tetapi juga mengubah paradigma interaksi antara parlemen dan publik menjadi lebih partisipatif dan berbasis data \textit{data driven}. 

Menurut \textcite{begany2024ogdtransformation}, transformasi digital di sektor publik menuntut pergeseran fokus dari sekadar digitalisasi dokumen, yaitu memindahkan kertas ke berkas \textit{PDF}, menjadi digitalisasi data, yaitu membuat informasi terstruktur dan dapat diolah mesin. Dalam ekosistem parlemen digital, transparansi diukur dari seberapa mudah data legislatif seperti \textit{event log} rapat, teks rancangan undang undang, dan rekam jejak \textit{voting} dapat diakses, ditelusuri, dan diaudit kembali oleh algoritma komputer maupun masyarakat luas tanpa hambatan teknis, sehingga memungkinkan pengawasan yang bersifat \textit{real time} dan berskala besar.

\subsection{Open Government Data (OGD) dalam Konteks Legislatif}
\textit{Open Government Data} \textit{OGD} didefinisikan sebagai data pemerintah yang dapat diakses secara bebas, digunakan ulang, dan didistribusikan kembali oleh siapa saja tanpa batasan hak cipta atau kontrol paten. Dalam konteks legislatif, penerapan prinsip \textit{OGD} bertujuan untuk memecahkan masalah asimetri informasi dengan menyediakan data mentah \textit{raw data} yang berkualitas tinggi kepada publik.

Berdasarkan tinjauan literatur sistematis yang dilakukan \textcite{kempeneer2023ogdlegal}, data parlemen yang memenuhi standar \textit{OGD} harus memiliki karakteristik lengkap, primer atau bersumber langsung dari pencatat, tepat waktu, dapat diakses, dapat diproses mesin \textit{machine processable}, non diskriminatif, non proprietary, dan bebas lisensi \autocite{kempeneer2023ogdlegal}. Penerapan standar ini krusial untuk memungkinkan pengembangan aplikasi pihak ketiga \textit{civic tech} yang dapat mengolah data parlemen yang kompleks menjadi informasi yang mudah dipahami, sekaligus memfasilitasi audit otomatis terhadap kinerja lembaga perwakilan.

\subsection{Teknologi Sipil \textit{Civic Tech} dan Pemantauan Parlemen}
Teknologi sipil \textit{Civic Tech} merujuk pada penggunaan teknologi digital untuk meningkatkan partisipasi warga, memperkuat infrastruktur demokrasi, dan mendorong efektivitas pemerintahan. Dalam domain legislatif, \textit{Civic Tech} terwujud melalui platform organisasi pemantau parlemen \textit{Parliamentary Monitoring Organizations (PMO)} yang berfungsi sebagai perantara informasi antara parlemen dan warga. Platform ini mengagregasi data parlemen yang kompleks dan menyajikannya dalam bentuk visualisasi yang lebih mudah dipahami publik. 

Penelitian \textcite{halimatusadiyah2025understanding} menyoroti bahwa \textit{Civic Tech} berperan penting dalam mengisi kekosongan saluran komunikasi politik dan mengubah partisipasi pasif menjadi aktivisme digital yang terinformasi \autocite{halimatusadiyah2025understanding}. Berbeda dengan portal resmi pemerintah yang sering bersifat kaku, inisiatif \textit{Civic Tech} berfokus pada pengalaman pengguna \textit{user experience} untuk memudahkan warga melakukan audit sosial terhadap kinerja wakil rakyat mereka.

\subsection{Partisipasi Digital Publik dalam Pengawasan Legislatif}
Partisipasi publik digital \textit{e participation} didefinisikan sebagai keterlibatan warga dalam proses pemerintahan dan pengambilan kebijakan melalui sarana teknologi informasi dan komunikasi. Dalam konteks pengawasan legislatif, \textit{e participation} melampaui sekadar akses informasi menuju interaksi dua arah, di mana warga dapat memberikan umpan balik, kritik, dan penilaian terhadap kinerja wakil rakyat. 

Studi yang dilakukan \textcite{kreps2023can} menunjukkan bahwa alat komunikasi digital yang dirancang dengan baik dapat meningkatkan responsivitas legislatif secara signifikan karena legislator merasa diawasi secara \textit{real time} oleh konstituennya \autocite{kreps2023can}. Namun, efektivitas partisipasi ini sangat bergantung pada kualitas informasi yang disajikan oleh platform \textit{Civic Tech}. Informasi yang dangkal hanya menghasilkan partisipasi simbolis, sedangkan informasi yang terstruktur dan berbasis bukti, misalnya perbandingan antara janji dan realisasi, mampu mendorong partisipasi substantif yang memperkuat akuntabilitas vertikal.


%II.3 Pentingnya Transparansi Parlemen Digital======================================================
\section{Urgensi Transparansi Parlemen Digital}
Ketersediaan data yang terbuka dan terstandarisasi telah menjadi variabel penting dalam membangun ekosistem demokrasi yang responsif. Penelitian oleh \textcite{chen2023determinants} menunjukkan bahwa tingkat kepercayaan masyarakat terhadap lembaga politik dapat meningkat hingga 28 persen di lingkungan yang menerapkan \textit{open government data} secara konsisten. Di sisi lain, laporan empiris \textcite{begany2024ogdtransformation} menemukan bahwa adopsi praktik digitalisasi transparansi, melalui portal data terbuka parlemen, berkorelasi positif dengan peningkatan tingkat partisipasi publik dalam pengawasan legislatif, dengan jumlah kontribusi umpan balik warga melonjak dua kali lipat selama dua tahun pertama inisiatif tersebut dijalankan.

Dampak sistemik dari kurangnya transparansi digital dapat diukur melalui persepsi korupsi dan pola partisipasi politik yang stagnan. \textcite{ferry2024democracy} melaporkan bahwa institusi yang gagal menyediakan akses akuntabilitas digital mengalami peningkatan 19 persen dalam indeks persepsi korupsi menurut survei publik nasional. Sementara itu, \textcite{bordignon2020rules} menggarisbawahi bahwa adanya mekanisme pengawasan digital mendorong kedisiplinan perilaku anggota parlemen, terlihat dari penurunan kasus pelanggaran prosedural sebesar 14 persen setelah diterapkannya sistem pemantauan terkomputerisasi. Tidak hanya berdampak bagi masyarakat umum, transparansi digital juga memberikan landasan bagi organisasi masyarakat sipil dan media untuk memverifikasi rekam jejak legislator secara independen, sehingga meningkatkan efektivitas kontrak sosial antara wakil rakyat dengan para pemilihnya.

Jika institusi politik gagal mengadopsi standar keterbukaan digital ini, risiko jangka panjangnya adalah menurunnya legitimasi dan fragmentasi kepercayaan publik, yang dalam jangka waktu tertentu berpotensi melemahkan stabilitas demokrasi \textcite{rienks2023corruption}. Oleh sebab itu, urgensi transparansi digital tidak hanya berakar pada norma etik dan hukum, tetapi juga pada hasil nyata berupa perbaikan kualitas hubungan antara warga dan parlemen yang terukur secara empiris.

%II.3.1 Hubungan Transparansi Digital dengan Kepercayaan Publik dan Legitimasi
\subsection{Hubungan Transparansi Digital dengan Kepercayaan Publik dan Legitimasi}
Transparansi digital parlemen berkaitan erat dengan pembentukan dan pemeliharaan kepercayaan publik terhadap lembaga perwakilan. Seiring meningkatnya literasi digital warga, ekspektasi terhadap keterbukaan informasi juga bergeser dari sekadar akses dokumen menuju ketersediaan data yang dapat ditelusuri dan diverifikasi secara mandiri. Studi kuantitatif menunjukkan bahwa keberadaan kanal transparansi digital yang aktif dapat meningkatkan persepsi kepercayaan publik terhadap lembaga politik secara signifikan dibandingkan lembaga yang hanya mengandalkan mekanisme pelaporan konvensional \autocite{chen2023determinants}. Dalam konteks ini, transparansi tidak hanya berfungsi sebagai kewajiban administratif, tetapi juga sebagai mekanisme komunikasi simbolik yang menandakan kesediaan lembaga untuk diawasi.

Di sisi lain, legitimasi parlemen sebagai institusi demokratis juga sangat dipengaruhi oleh sejauh mana proses dan hasil kerjanya dapat diakses secara terbuka melalui platform digital. \textcite{ferry2024democracy} menunjukkan bahwa warga cenderung menganggap lembaga politik lebih sah, atau \textit{legitimate}, ketika mereka memiliki kesempatan yang jelas untuk memantau dan mengevaluasi kinerja wakil rakyat secara berkelanjutan. Ketika informasi mengenai agenda sidang, hasil pemungutan suara, serta partisipasi anggota legislatif tersedia secara transparan dan mudah diakses, warga merasakan adanya saluran akuntabilitas yang nyata sehingga memperkuat keyakinan bahwa sistem demokrasi bekerja sesuai prinsip representasi. Sebaliknya, ketiadaan transparansi digital yang memadai membuka ruang bagi tumbuhnya kecurigaan, spekulasi negatif, dan erosi kepercayaan yang pada akhirnya dapat melemahkan legitimasi lembaga perwakilan itu sendiri \autocite{rienks2023corruption}.
%II.3.2 Kebutuhan Data Terstruktur untuk Pengawasan Berkelanjutan
\subsection{Kebutuhan Data Terstruktur untuk Pengawasan Berkelanjutan}

Transparansi parlemen digital hanya dapat berfungsi secara nyata jika informasi yang disajikan tidak berhenti pada tingkat dokumen, tetapi diolah menjadi data yang terstruktur, konsisten, dan mudah dikaitkan lintas waktu. Studi literatur mengenai desain transparansi digital menunjukkan bahwa tanpa struktur data yang jelas, upaya membuka informasi justru berisiko menciptakan apa yang disebut sebagai “transparansi semu”, yaitu kondisi ketika volume informasi yang tersedia sangat besar tetapi sulit dipakai ulang untuk tujuan pengawasan \autocite{wirtz2022opengovdata}. Data yang terstruktur memungkinkan aktivitas legislatif direkam sebagai jejak yang berkelanjutan sehingga pola perilaku, perubahan sikap, dan konsistensi posisi terhadap suatu isu dapat diamati secara longitudinal, bukan hanya sebagai potret sesaat \autocite{kempeneer2023ogdlegal}.

Sebaliknya, ketiadaan struktur data yang memadai membuat pengawasan publik bergantung pada pembacaan manual yang terputus putus dan sangat rentan bias. \textcite{begany2024ogdtransformation} menunjukkan bahwa inisiatif data terbuka yang tidak menyediakan format terstruktur dan dapat diproses mesin cenderung kurang dimanfaatkan oleh pengguna lanjutan seperti peneliti dan organisasi masyarakat sipil, sehingga potensi akuntabilitasnya tidak tercapai secara optimal. Ketika data aktivitas dan keputusan legislator disusun secara sistematis dan dapat dipantau dari waktu ke waktu, mekanisme akuntabilitas menjadi lebih kuat karena setiap perubahan posisi, ketidakhadiran, atau penyimpangan dari pola sebelumnya dapat segera terdeteksi dan dipertanyakan \autocite{hong2024transparency}.

Dengan demikian, kebutuhan akan data yang terstruktur dan dapat diawasi secara berkelanjutan bukan hanya persoalan teknis, tetapi merupakan prasyarat agar prinsip transparansi digital benar benar dapat diterjemahkan menjadi pengawasan yang efektif dan berkesinambungan \autocite{wirtz2022opengovdata}.

%II.4 Pendekatan Teknis Global dalam Pemantauan Kinerja Legislatif
\section{Pendekatan Teknis Global dalam Pemantauan Kinerja Legislatif}

Dalam lanskap penelitian global, pendekatan pemantauan kinerja legislatif mulai bergeser dari pembacaan dokumen secara manual menuju penggunaan metode komputasional yang memanfaatkan data dalam skala besar. Sejalan dengan berkembangnya korpus digital dan inisiatif \textit{open data}, berbagai studi menggabungkan teknik pemrosesan bahasa alami, analisis peristiwa berbasis teks, dan kerangka kerja data terbuka untuk mengekstraksi pola perilaku legislatif yang sebelumnya sulit diobservasi. \textcite{liu2023surveynewsnarrative} menunjukkan bahwa paradigma ekstraksi peristiwa berbasis teks memungkinkan rangkaian kejadian politik direkonstruksi sebagai narasi terstruktur yang siap dianalisis secara kuantitatif. Pendekatan ini memberikan landasan bagi penelitian lanjutan di Eropa dan Amerika Utara yang memanfaatkan data parlemen terstruktur sebagai bahan baku untuk membangun akuntabilitas yang lebih terukur.

\subsection{Studi dan Praktik di Eropa}

Di kawasan Eropa, pengembangan korpus sidang parlemen yang terstandardisasi menjadi salah satu fondasi utama bagi informatika parlemen modern. \textcite{erjavec2023bcorpora} mengembangkan \textit{ParlaMint}, yaitu kumpulan besar transkrip sidang parlemen multibahasa yang dianotasi secara linguistik dan disusun dalam format terstruktur. Korpus ini memungkinkan peneliti melakukan analisis yang konsisten lintas negara, misalnya memetakan pola intervensi anggota parlemen, intensitas pembahasan isu tertentu, dan perubahan retorika politik dari waktu ke waktu. Dengan adanya struktur data yang seragam, proses ekstraksi entitas, relasi, dan peristiwa dari teks sidang dapat dilakukan secara lebih sistematis dan dapat direplikasi.

Selain korpus, kerangka kerja analisis data parlemen juga mulai berkembang. \textcite{berkovitz2023openframework} mengusulkan sebuah kerangka terbuka untuk menganalisis data parlemen publik yang menggabungkan prinsip \textit{big data}, visualisasi, dan keterbukaan data. Kerangka ini dirancang untuk memudahkan integrasi berbagai sumber data legislatif seperti metadata rancangan undang undang, rekam jejak \textit{voting}, dan daftar keanggotaan komisi ke dalam satu sistem yang dapat diolah dan divisualisasikan. 

Dari sudut pandang penelitian ini, praktik di Eropa memberikan contoh konkret tentang bagaimana data sidang yang semula berupa teks naratif dapat diubah menjadi korpus terstruktur yang mendukung analisis perilaku legislatif. Namun, fokus utamanya masih pada pemetaan aktivitas dan pola wacana, belum secara spesifik menghubungkan aktivitas tersebut dengan janji kampanye individual sebagaimana yang diupayakan dalam proyek ini.

\subsection{Studi dan Praktik di Amerika Utara}

Di Amerika Utara, ekosistem pemantauan legislatif berkembang di atas kombinasi antara portal resmi pemerintah dan inisiatif teknologi sipil. Di tingkat federal Amerika Serikat, \textcite{loc_congressgov} menyediakan \textit{Congress.gov} sebagai portal resmi legislatif yang menyajikan data rancangan undang undang, riwayat proses legislasi, serta informasi anggota Kongres dalam format yang dapat diakses publik. Panduan yang disusun oleh \textcite{loc_nodate_congressdata} menjelaskan bahwa data \textit{Congress.gov} dapat diunduh dan digunakan secara \textit{offsite}, termasuk dalam bentuk \textit{bulk data}, sehingga memungkinkan pengembang dan peneliti membangun aplikasi pemantauan berbasis data legislatif yang lebih canggih.

Di atas infrastruktur data resmi tersebut, platform independen seperti \textit{GovTrack.us} memanfaatkan data \textit{Congress.gov} untuk menyajikan rekam jejak anggota Kongres dalam bentuk yang lebih mudah dipahami publik. \textcite{govtrack_about} menjelaskan bahwa \textit{GovTrack} mengolah data \textit{voting}, keanggotaan komite, dan riwayat legislasi untuk menghasilkan metrik seperti produktivitas, kedekatan ideologis, dan pola \textit{co sponsorship}. Pendekatan ini menunjukkan bagaimana data legislatif yang disediakan pemerintah dapat ditransformasikan menjadi \textit{dashboard} pemantauan yang lebih bersahabat bagi warga. 

Di sisi lain, survei yang dilakukan oleh \textcite{liu2023surveynewsnarrative} tentang ekstraksi narasi berbasis peristiwa memperlihatkan bagaimana teknik \textit{event based analysis} digunakan untuk menyusun kembali rangkaian peristiwa politik dari teks berita, yang secara konseptual sejalan dengan upaya membangun log aktivitas legislatif dari dokumen sidang.

Dari perspektif penelitian ini, praktik di Amerika Utara memberikan inspirasi kuat mengenai pentingnya portal data resmi yang menyediakan data terstruktur dan dapat diakses, serta peran kunci platform perantara seperti \textit{GovTrack} dalam menerjemahkan data tersebut menjadi informasi yang bermakna bagi publik. Namun, seperti halnya di Eropa, fokus utama ekosistem ini masih berpusat pada pemantauan aktivitas dan \textit{voting}, sementara aspek penautan eksplisit antara janji kampanye individu dengan aktivitas legislatif harian belum menjadi perhatian utama. Ruang kosong inilah yang kemudian menjadi titik masuk bagi kontribusi penelitian ini.

%II.5 Studi dan Praktik Transparansi Parlemen di Indonesia
\section{Studi dan Praktik Transparansi Parlemen di Indonesia}

Dalam konteks Indonesia, upaya mewujudkan transparansi parlemen berjalan melalui dua jalur utama, yaitu mekanisme resmi kelembagaan dan inisiatif teknologi sipil \textit{civic tech}. Literatur mengenai tata kelola data pemerintah menunjukkan bahwa kebijakan dan infrastruktur digital di Indonesia masih didominasi pendekatan \textit{e government} umum dan layanan administrasi, sementara pemanfaatan data untuk pemantauan kinerja legislatif pasca pemilu relatif tertinggal \autocite{breuer2023integrated}. 

Di satu sisi, DPR RI dan lembaga terkait telah mengembangkan berbagai sistem informasi untuk menyajikan dokumen legislasi dan proses pembahasan. Di sisi lain, platform independen seperti \textit{JariUngu} berusaha mengisi celah keterbukaan dengan menyajikan profil dan informasi representasi politik kepada publik \autocite{jariungu_tentangkami}. Kedua jalur ini penting untuk dipahami guna melihat sejauh mana ekosistem yang ada saat ini sudah mendukung akuntabilitas kinerja anggota dewan.

\subsection{Transparansi Kelembagaan DPR RI}

Secara kelembagaan, DPR RI telah mengembangkan berbagai instrumen formal untuk mendukung keterbukaan proses legislasi. Salah satu kerangka penting adalah Sistem Informasi Legislasi \textit{SILEG} yang diatur melalui pedoman Sekretaris Jenderal DPR RI dan dirancang untuk mendokumentasikan tahapan proses pembentukan undang undang, mulai dari perencanaan, pembahasan, hingga pengesahan \autocite{sekjen2020sileg}. 

Melalui sistem ini, publik dapat mengakses dokumen seperti naskah rancangan undang undang, jadwal rapat, dan ringkasan proses legislasi. Selain itu, Jaringan Dokumentasi dan Informasi Hukum \textit{JDIH} dan situs resmi DPR menyediakan produk hukum final serta informasi umum mengenai alat kelengkapan dewan dan keanggotaan.

Namun, temuan \textcite{breuer2023integrated} menyoroti bahwa pada tataran praktik, tata kelola data di Indonesia masih menghadapi fragmentasi kelembagaan dan keterbatasan kualitas infrastruktur. Kondisi ini tercermin dalam ekosistem informasi DPR, di mana data legislatif tersebar di berbagai portal dan sering kali tersaji dalam format dokumen statis yang tidak terstruktur. Ketiadaan skema data terpadu membuat publik atau peneliti harus melakukan penelusuran manual jika ingin melacak aktivitas seorang anggota dewan lintas rapat, komisi, dan periode waktu. 

Belum terdapat satu struktur data yang secara eksplisit merepresentasikan siapa melakukan apa, kapan, dan dalam konteks apa, sehingga sulit membangun \textit{event log} yang komprehensif dari sumber resmi yang ada. Lebih jauh, tidak ada mekanisme yang menghubungkan secara langsung antara data aktivitas legislatif ini dengan dokumen janji kampanye, sehingga akuntabilitas terhadap mandat elektoral belum terwujud dalam bentuk data yang dapat dianalisis secara sistematis.

\subsection{Inisiatif Civic Tech: Studi Kasus \textit{JariUngu}}

Di luar jalur resmi kelembagaan, Indonesia juga menyaksikan berkembangnya inisiatif teknologi sipil yang berupaya menjembatani kesenjangan informasi antara warga dan wakil rakyat. Salah satu contoh menonjol adalah \textit{JariUngu}, sebuah platform \textit{web} yang berfungsi sebagai direktori informasi politik dan legislasi. Berdasarkan penjelasan pada laman \textit{Tentang Kami}, \textit{JariUngu} menyajikan berbagai fitur seperti profil anggota DPR dan DPRD, data calon legislatif, ringkasan regulasi, serta fasilitas pencarian berdasarkan nama atau daerah pemilihan \autocite{jariungu_tentangkami}. 

Dengan cara ini, \textit{JariUngu} membantu pemilih untuk mengenali siapa wakil mereka, apa latar belakangnya, dan regulasi apa saja yang terkait. Studi mengenai aktivisme digital di Indonesia menunjukkan bahwa platform seperti ini berkontribusi dalam menurunkan hambatan informasi dan mendorong partisipasi politik yang lebih terinformasi \autocite{halimatusadiyah2025understanding}.

Meskipun demikian, dari perspektif pemantauan kinerja legislatif pasca pemilu, pendekatan yang diambil \textit{JariUngu} masih memiliki keterbatasan struktural. Fokus utama platform ini adalah pada penyediaan informasi profil dan data elektoral, sedangkan pencatatan aktivitas harian legislator seperti intervensi dalam rapat, pola kehadiran, atau posisi terhadap isu tertentu belum dimodelkan sebagai rangkaian peristiwa terstruktur. Data yang tersedia lebih mendekati katalog informasi statis daripada \textit{event log} yang memungkinkan analisis jejak aktivitas secara kronologis. Selain itu, belum terdapat mekanisme teknis yang secara eksplisit menautkan janji kampanye individual dengan rekam tindakan legislatif yang terdokumentasi, sehingga publik masih harus menyimpulkan hubungan tersebut secara manual. 

Dengan demikian, meskipun \textit{JariUngu} menunjukkan bahwa terdapat kebutuhan dan manfaat nyata dari platform pemantauan legislatif, masih terdapat ruang signifikan untuk pengembangan arsitektur data yang secara khusus dirancang untuk menghubungkan janji politik dan kinerja aktual anggota DPR RI, yang menjadi fokus utama penelitian ini.

%II.6 Tantangan Teknis dalam Pemrosesan Data Legislatif Indonesia
\section{Tantangan Teknis dalam Pemrosesan Data Legislatif Indonesia}

Meskipun terdapat perkembangan signifikan dalam informatika parlemen di tingkat global, penerapan teknologi serupa di Indonesia menghadapi sejumlah tantangan teknis spesifik yang membedakan konteks DPR RI dari praktik terbaik internasional. 

Tantangan utama terletak pada fragmentasi dan heterogenitas format data. Berbeda dengan inisiatif \textit{ParlaMint} di Eropa yang menyediakan korpus parlemen multibahasa dalam format \textit{XML TEI} yang terstandarisasi \autocite{erjavec2023bcorpora}, atau \textit{Congress.gov} di Amerika Serikat yang menyediakan akses data massal \textit{bulk data} siap pakai, data legislatif Indonesia tersebar di berbagai subdomain terpisah seperti \textit{berkas.dpr.go.id}, \textit{dpr.go.id}, dan \textit{jdih.dpr.go.id} tanpa skema data terpadu. Dokumen dokumen ini tersaji dalam format campuran yang tidak konsisten, mulai dari \textit{PDF} hasil pemindaian \textit{scanned PDF} untuk risalah lama, \textit{PDF} berbasis teks untuk laporan singkat, hingga tabel \textit{HTML} untuk jadwal kegiatan. Kondisi ini menjadikan tahap penemuan dokumen \textit{Information Retrieval} sebagai langkah kritis yang harus dilakukan sebelum ekstraksi informasi dapat dijalankan, suatu kompleksitas yang jarang ditemui dalam penelitian yang menggunakan korpus siap pakai. Fragmentasi kelembagaan dalam pengelolaan data ini menghambat pemanfaatan data untuk analisis komputasional yang presisi.

Selain masalah format, tantangan metodologis terbesar dalam pemantauan kinerja DPR RI adalah ketiadaan catatan pemungutan suara individu \textit{individual roll call votes}. Berbeda dengan sistem di Kongres Amerika Serikat di mana posisi setiap anggota tercatat secara eksplisit dalam setiap \textit{voting}, mayoritas pengambilan keputusan di DPR RI dilakukan berdasarkan musyawarah mufakat atau diwakili oleh pandangan fraksi. Catatan resmi sering kali hanya menyatakan posisi agregat, misalnya tujuh fraksi menyetujui dan dua fraksi menolak, tanpa merinci siapa saja anggota yang hadir dan mendukung keputusan tersebut secara individu. 

Keterbatasan data dasar ini menuntut pengembangan model inferensi sikap politik \textit{stance inference} yang tidak bergantung pada \textit{ground truth} langsung, melainkan diturunkan dari kombinasi keanggotaan fraksi, kehadiran dalam rapat, dan posisi resmi fraksi terkait. Pendekatan inferensial ini merupakan adaptasi metodologis yang diperlukan untuk mengatasi kekosongan data \textit{voting} individual yang juga tidak tersedia di platform pemantau eksternal seperti \textit{JariUngu}.

Terakhir, terdapat kesenjangan sumber daya linguistik untuk domain legislatif bahasa Indonesia. Inisiatif seperti \textit{ParlaMint} telah menyediakan korpus yang dianotasi secara linguistik untuk mengenali entitas bernama \textit{Named Entity Recognition (NER)}, struktur kalimat, dan metadata pembicara secara otomatis. Di Indonesia, meskipun telah tersedia model bahasa umum seperti \textit{IndoBERT} \autocite{koto2020indolem} atau \textit{benchmark} \textit{IndoNLU} \autocite{wilie2020indonlu}, belum ada korpus risalah DPR yang secara spesifik dianotasi untuk mengenali entitas legislatif seperti nama anggota, nomor rancangan undang undang, nama alat kelengkapan dewan, atau referensi pasal hukum yang kompleks. 

Model \textit{Natural Language Processing (NLP)} umum sering kali gagal menangkap struktur wacana dan terminologi khas parlemen tanpa penyesuaian khusus \textit{domain adaptation}. Oleh karena itu, penelitian ini tidak dapat sekadar menerapkan perangkat lunak \textit{NLP} yang sudah ada, tetapi perlu membangun komponen ekstraksi kustom \textit{custom parsers} dan pola pencocokan \textit{pattern matching} yang mampu menangani kompleksitas linguistik dalam dokumen DPR RI. Dengan mempertimbangkan tantangan tantangan teknis ini, yaitu fragmentasi data, inferensi sikap politik, dan ketiadaan korpus anotasi legislatif, penelitian ini mengusulkan pendekatan arsitektur teknis yang disesuaikan dengan realitas data DPR RI, yang akan dijabarkan pada bagian berikutnya.
% II.7 
\section{Pendekatan Teknis Sistem \textit{Legislative Activity Tracker}}

Pengembangan sistem pemantauan legislatif \textit{Legislative Activity Tracker} menuntut pendekatan teknis yang mampu menjembatani kesenjangan antara data tidak terstruktur yang tersedia di domain publik dengan kebutuhan akan informasi yang terstruktur dan dapat diaudit. Mengingat karakteristik data DPR RI yang heterogen, tersebar, dan didominasi oleh format naratif, solusi konvensional berbasis basis data relasional semata tidak lagi memadai. Oleh karena itu, penelitian ini mengadopsi pendekatan komputasional berbasis kecerdasan buatan, khususnya dalam ranah pemrosesan teks dan pencarian informasi, untuk mentransformasi tumpukan dokumen mentah menjadi pengetahuan yang bermakna.

Pendekatan teknis yang diusulkan dirancang secara berlapis \textit{layered architecture}, dimulai dari akuisisi dan pemahaman konten dokumen menggunakan teknik pemrosesan bahasa, diikuti dengan ekstraksi struktur peristiwa yang merepresentasikan aktivitas legislatif, hingga mekanisme penautan logis untuk menghubungkan aktivitas tersebut dengan konteks mandat politiknya. Strategi ini dipilih untuk memastikan bahwa sistem tidak hanya berfungsi sebagai arsip digital pasif, melainkan sebagai instrumen analitis aktif yang mampu memodelkan realitas institusional DPR RI, di mana dinamika fraksi dan partisipasi individu saling terkait, ke dalam format data yang siap diolah oleh mesin dan diakses oleh publik.

\subsection{Pemrosesan Bahasa Alami \textit{Natural Language Processing (NLP)}}

Pemrosesan Bahasa Alami \textit{Natural Language Processing (NLP)} merupakan domain komputasional yang memanfaatkan teknik pembelajaran mendalam \textit{deep learning} untuk memodelkan struktur linguistik kompleks ke dalam representasi vektor matematis. Perkembangan mutakhir dalam \textit{NLP} didorong oleh adopsi arsitektur \textit{Transformer} yang diperkenalkan oleh Vaswani dan kolega, yang menggantikan model sekuensial tradisional seperti \textit{Recurrent Neural Networks (RNN)} dengan mekanisme atensi diri \textit{self attention mechanism}. Secara matematis, mekanisme ini menghitung bobot hubungan antar token dalam sebuah sekuens input $X$ melalui transformasi linear menjadi tiga matriks, yaitu \textit{Query} ($Q$), \textit{Key} ($K$), dan \textit{Value} ($V$). Skor atensi dihitung menggunakan fungsi
\begin{equation}
Attention(Q, K, V) = softmax\!\left(\frac{QK^T}{\sqrt{d_k}}\right)V,
\label{eq:contoh1}
\end{equation}
di mana $d_k$ adalah dimensi dari vektor \textit{key}. Mekanisme ini memungkinkan model untuk memproses seluruh sekuens secara paralel dan menangkap ketergantungan jarak jauh \textit{long range dependencies} dengan efisiensi komputasi yang jauh lebih tinggi dibandingkan \textit{RNN} \autocite{islam2024survey}.

Dalam implementasi praktis, model berbasis \textit{Transformer} seperti \textit{BERT} \textit{Bidirectional Encoder Representations from Transformers} telah menjadi standar de facto untuk berbagai tugas hilir \textit{NLP}. \textcite{lin2022survey} menjelaskan bahwa arsitektur \textit{encoder decoder} pada \textit{Transformer} memungkinkan model untuk mempelajari representasi kontekstual dua arah \textit{bidirectional}, yang secara signifikan meningkatkan performa dalam tugas pemahaman bahasa. 

Studi kuantitatif oleh \textcite{lu2025measuring} menunjukkan bahwa penggunaan model pra latih seperti \textit{SciBERT} yang dikombinasikan dengan lapisan \textit{BiLSTM} \textit{Bidirectional Long Short Term Memory} mampu mencapai skor F1 sebesar 86.27 persen dalam tugas pengenalan entitas pada teks ilmiah, mengungguli model dasar \textit{BERT} standar dengan skor F1 sebesar 84.91 persen dan \textit{BERT+CRF} dengan skor F1 sebesar 83.15 persen. Peningkatan performa ini diatribusikan pada kemampuan model untuk menghasilkan \textit{word embeddings} yang dinamis, di mana setiap kata direpresentasikan sebagai vektor padat \textit{dense vector} dalam ruang multidimensi $V$, sehingga kata kata dengan kedekatan semantik memiliki jarak \textit{Euclidean} atau \textit{cosine similarity} yang minimal \autocite{rani2021weighted}.

Efektivitas model \textit{NLP} modern juga sangat bergantung pada teknik pembelajaran transfer \textit{transfer learning}, di mana model yang telah dilatih pada korpus berukuran besar \textit{pre training} diadaptasi untuk tugas spesifik melalui proses \textit{fine tuning}. \textcite{alanazi2022nlp} melaporkan bahwa teknik augmentasi data dan \textit{fine tuning} pada model pembelajaran mendalam dapat meningkatkan skor F1 antara satu persen sampai delapan persen dalam tugas klasifikasi teks medis dibandingkan model tanpa augmentasi. 

Namun, kompleksitas komputasi model \textit{Transformer} yang berskala kuadratik terhadap panjang sekuens input $O(n^2)$ tetap menjadi tantangan utama, terutama untuk pemrosesan dokumen panjang. Oleh karena itu, varian efisien seperti \textit{Sparse Transformers} atau teknik kompresi model seperti \textit{pruning} dan \textit{quantization} terus dikembangkan untuk menyeimbangkan antara akurasi prediktif dan efisiensi memori \autocite{islam2024survey}.

\subsubsection{Arsitektur Model Neural untuk Pemrosesan Teks: \textit{BERT} dan \textit{ColBERT}}

Dalam lanskap pemrosesan bahasa alami modern, model berbasis \textit{Transformer} telah menjadi standar de facto untuk menangkap semantik teks yang kompleks. Arsitektur yang paling berpengaruh adalah \textit{BERT} \textit{Bidirectional Encoder Representations from Transformers}, yang diperkenalkan sebagai model yang memanfaatkan \textit{multi layer bidirectional Transformer encoders}. Tidak seperti model sekuensial tradisional yang memproses teks secara searah, \textit{BERT} dilatih menggunakan objektif \textit{Masked Language Modeling (MLM)} yang memungkinkan model untuk mempelajari representasi kontekstual mendalam dari kedua arah, yaitu dari kiri ke kanan dan dari kanan ke kiri, secara simultan. 

Studi oleh \textcite{lin2022survey} menjelaskan bahwa arsitektur ini memungkinkan \textit{BERT} untuk menghasilkan \textit{embeddings} yang sensitif terhadap konteks, di mana representasi vektor dari sebuah kata berubah tergantung pada kata kata di sekitarnya. Varian standar \textit{BERT Base} memiliki dua belas lapisan \textit{encoder}, dimensi tersembunyi \textit{hidden size} sebesar tujuh ratus enam puluh delapan, dan total seratus sepuluh juta parameter, yang secara empiris terbukti mencapai performa \textit{state of the art} dalam berbagai tugas pemahaman bahasa \autocite{lin2022survey}. Namun, keterbatasan utama \textit{BERT} terletak pada biaya komputasi yang tinggi untuk pencarian informasi \textit{information retrieval} skala besar, karena mekanisme \textit{full self attention} memiliki kompleksitas kuadratik terhadap panjang input.

Untuk mengatasi inefisiensi tersebut dalam konteks pencarian informasi, arsitektur \textit{ColBERT} \textit{Contextualized Late Interaction over BERT} dikembangkan sebagai solusi hibrida yang mempertahankan kemampuan semantik \textit{BERT} namun dengan efisiensi pencarian yang jauh lebih tinggi. \textcite{wang2022improving} menjelaskan bahwa \textit{ColBERT} memperkenalkan paradigma \textit{late interaction}, di mana \textit{query} dan dokumen dikodekan secara terpisah menjadi representasi vektor padat \textit{dense vectors} menggunakan \textit{BERT}, dan interaksi relevansi dihitung hanya pada tahap akhir menggunakan operasi \textit{MaxSim}, yaitu maksimum kesamaan antar vektor token. 

Secara kuantitatif, pendekatan ini memungkinkan \textit{ColBERT} untuk melakukan pencarian dokumen dengan latensi yang rendah dalam skala milidetik sambil mempertahankan akurasi yang kompetitif dengan model interaksi penuh yang jauh lebih lambat. Evaluasi eksperimental menunjukkan bahwa pendekatan \textit{dense retrieval} seperti \textit{ColBERT} mampu meningkatkan efektivitas pencarian \textit{passage retrieval} hingga seratus tiga puluh enam persen dibandingkan metode \textit{sparse retrieval} tradisional dalam skenario tertentu \autocite{li2023generative}. Keunggulan ini menjadikan \textit{ColBERT} pilihan arsitektural yang ideal untuk sistem yang membutuhkan keseimbangan antara pemahaman semantik mendalam dan kecepatan eksekusi pada korpus dokumen yang besar.

\subsubsection{Model Bahasa dan Sumber Daya Linguistik Indonesia}

Penerapan teknik \textit{NLP} pada bahasa dengan sumber daya terbatas \textit{low resource languages} seperti bahasa Indonesia menghadapi tantangan unik dibandingkan dengan bahasa Inggris yang memiliki ekosistem data melimpah. Untuk mengatasi hal ini, pengembangan sumber daya linguistik standar menjadi krusial. 

Salah satu inisiatif fundamental adalah \textit{IndoLEM} \textit{Indonesian Language Evaluation Montage}, sebuah kerangka kerja \textit{benchmark} komprehensif yang dirancang untuk mengevaluasi performa model bahasa Indonesia pada berbagai tugas \textit{NLP}, termasuk morfologi, sintaksis, dan semantik. \textcite{nabiilah2023bert} menjelaskan bahwa ketersediaan \textit{benchmark} terstandarisasi seperti \textit{IndoLEM} memungkinkan peneliti untuk mengukur efektivitas model secara objektif dan membandingkan arsitektur yang berbeda pada dataset yang seragam, yang mencakup tugas tugas seperti penandaan kelas kata \textit{POS tagging} dan pengenalan entitas bernama \textit{Named Entity Recognition (NER)}.

Sejalan dengan kebutuhan tersebut, model bahasa pra latih \textit{pre trained language models} khusus bahasa Indonesia telah dikembangkan untuk menangkap karakteristik linguistik lokal. \textit{IndoBERT}, sebuah varian \textit{BERT} yang dilatih pada korpus teks bahasa Indonesia berskala besar, termasuk \textit{Wikipedia} Indonesia, artikel berita, dan \textit{web corpus}, telah menjadi standar de facto untuk tugas tugas pemahaman bahasa Indonesia. 

Studi oleh \textcite{ekakristi2025intermediate} menunjukkan bahwa \textit{IndoBERT} secara konsisten mengungguli model multibahasa seperti \textit{mBERT} dalam tugas tugas yang memerlukan pemahaman nuansa lokal. Secara kuantitatif, \textit{IndoBERT} mencapai skor rata rata F1 sebesar delapan puluh delapan koma empat puluh tiga persen untuk tugas klasifikasi dan delapan puluh satu koma sembilan puluh dua persen untuk tugas \textit{sequence labeling} seperti \textit{NER} pada dataset \textit{IndoNLU}, menunjukkan superioritasnya dibandingkan model yang tidak dikhususkan untuk bahasa Indonesia. 

Lebih lanjut, penelitian \textcite{ibrohim2023hate} menyoroti bahwa efektivitas model seperti \textit{IndoBERT} dapat ditingkatkan lebih lanjut melalui teknik \textit{intermediate task transfer learning}, di mana model dilatih pada tugas perantara yang relevan sebelum diterapkan pada tugas akhir, yang terbukti meningkatkan robustitas model terhadap variasi data yang tidak terduga.

\subsubsection{Metodologi Pengenalan Entitas Bernama \textit{Named Entity Recognition}}

Pengenalan Entitas Bernama \textit{Named Entity Recognition (NER)} secara teknis diformulasikan sebagai masalah pelabelan sekuens \textit{sequence labeling problem}, di mana tujuannya adalah memetakan sekuens token input $X = (x_1, x_2, \dots, x_n)$ menjadi sekuens label $Y = (y_1, y_2, \dots, y_n)$ yang merepresentasikan kategori entitas. Metodologi konvensional untuk tugas ini sering kali menggunakan pendekatan \textit{Conditional Random Fields (CRF)} untuk memodelkan ketergantungan antar label yang berdekatan. 

Namun, perkembangan terkini telah bergeser ke arah arsitektur pembelajaran mendalam hibrida seperti \textit{Bi LSTM CRF}, yang menggabungkan kemampuan \textit{Bidirectional Long Short Term Memory (Bi LSTM)} untuk menangkap fitur kontekstual jarak jauh dengan lapisan \textit{CRF} di bagian akhir untuk mengoptimalkan prediksi label secara global \autocite{goyal2024named}. Arsitektur ini secara konsisten menunjukkan performa unggul karena kemampuannya memodelkan dependensi transisi label, misalnya label I ORG seharusnya mengikuti B ORG, secara eksplisit.

Dalam era \textit{Large Language Models}, pendekatan berbasis \textit{Transformer} telah merevolusi \textit{NER} dengan menggantikan lapisan \textit{Bi LSTM} dengan \textit{encoder} berbasis atensi diri \textit{self attention}. \textcite{jehangir2023survey} menjelaskan bahwa model seperti \textit{BERT NER} memanfaatkan representasi kontekstual dinamis untuk mengatasi ambiguitas makna kata, yang merupakan kelemahan utama dari pendekatan berbasis \textit{embedding} statis seperti \textit{Word2Vec}. 

Meskipun demikian, tantangan signifikan tetap ada dalam menangani entitas bersarang \textit{nested entities}, di mana satu entitas terkandung di dalam entitas lain, misalnya frasa Universitas Indonesia yang merupakan organisasi, namun kata Indonesia di dalamnya juga merupakan nama lokasi. Metode pelabelan sekuens standar seperti skema BIO sering gagal menangani struktur ini. 

Untuk mengatasi masalah tersebut, \textcite{ma2023sequence} mengusulkan mekanisme \textit{multi level topic aware} yang memungkinkan model untuk mengenali entitas pada berbagai tingkat granularitas secara simultan, serta pendekatan berbasis \textit{span based classification} yang memperlakukan setiap rentang teks sebagai kandidat entitas independen, bukan sekadar memberikan label pada setiap token. Selain itu, integrasi logika pengetahuan domain \textit{knowledge aware deep logic learning} telah terbukti meningkatkan interpretabilitas model \textit{NER}, memungkinkan sistem untuk tidak hanya memprediksi entitas tetapi juga menjelaskan keputusan berdasarkan aturan logika yang dipelajari \autocite{lin2025interpretable}.

\subsubsection{\textit{Information Extraction}}

Ekstraksi Informasi \textit{Information Extraction (IE)} adalah proses otomatisasi untuk mentransformasi data teks tidak terstruktur menjadi format data terstruktur, seperti basis data relasional atau graf pengetahuan. Secara teknis, \textit{IE} terdiri dari dua subtugas utama yang saling berkaitan, yaitu Ekstraksi Relasi \textit{Relation Extraction (RE)} dan Ekstraksi Peristiwa \textit{Event Extraction (EE)}. Ekstraksi Relasi bertujuan untuk mengidentifikasi hubungan semantik biner atau n arit antara entitas yang telah dikenali, misalnya hubungan \textit{subject verb object}. Metode terkini dalam \textit{RE} memanfaatkan struktur sintaksis kalimat, seperti pohon dependensi \textit{dependency trees}, untuk mengidentifikasi frasa kunci yang menjadi penghubung antar entitas. 

\textcite{liu2025improved} mengusulkan penggunaan algoritma deteksi komunitas pada pohon dependensi yang dikombinasikan dengan \textit{Graph Convolutional Networks (GCN)}, yang terbukti meningkatkan akurasi ekstraksi relasi dengan memodelkan konektivitas topologis antar kata secara lebih efektif dibandingkan model berbasis sekuensial murni.

Di sisi lain, Ekstraksi Peristiwa \textit{Event Extraction} merupakan tugas yang lebih kompleks karena melibatkan identifikasi pemicu peristiwa \textit{event trigger}, tipe peristiwa, serta argumen argumen pendukungnya, seperti aktor, waktu, dan lokasi, yang sering kali tersebar di seluruh dokumen \textit{document level extraction}. Pendekatan tradisional yang memproses kalimat per kalimat sering kali gagal menangkap konteks peristiwa yang lintas kalimat. 

Untuk mengatasi hal ini, \textcite{xu2025laap} mengembangkan metode \textit{Learning the Argument of An Entity with Event Prompts (LAAP)}, yang menggunakan teknik \textit{prompting} dengan \textit{placeholders} untuk memandu model bahasa besar \textit{Large Language Models (LLMs)} dalam mengenali tipe peristiwa dan argumennya secara simultan di tingkat dokumen. Selain itu, pergeseran paradigma menuju pembelajaran gabungan \textit{joint learning} semakin dominan, di mana tugas pengenalan entitas dan ekstraksi relasi dilakukan secara bersamaan dalam satu kerangka kerja terpadu \textit{end to end} untuk menghindari propagasi kesalahan \textit{error propagation} dari metode pipa bertahap \textit{pipeline methods}.
\subsubsection{\textit{Information Retrieval}}

Temu Kembali Informasi \textit{Information Retrieval (IR)} adalah domain penelitian yang berfokus pada pengembangan algoritma untuk menemukan dokumen relevan dari koleksi data berskala besar berdasarkan kueri pengguna. Secara historis, model \textit{IR} didominasi oleh pendekatan \textit{sparse retrieval} berbasis leksikal seperti \textit{BM25}, yang menghitung skor relevansi berdasarkan frekuensi kata kunci yang tumpang tindih antara kueri dan dokumen. 

Namun, keterbatasan metode ini dalam menangkap makna semantik, yang sering disebut sebagai masalah \textit{vocabulary mismatch}, telah mendorong transisi menuju pendekatan \textit{dense retrieval} berbasis model \textit{neural}. \textcite{guo2020deep} dalam tinjauan komprehensifnya menjelaskan bahwa model perankingan \textit{neural} \textit{neural ranking models} memanfaatkan jaringan saraf tiruan untuk mempelajari representasi vektor dari kueri dan dokumen, memungkinkan sistem untuk mencocokkan relevansi berdasarkan kedekatan semantik laten, bukan sekadar pencocokan kata literal. 

Arsitektur seperti \textit{Dense Passage Retrieval (DPR)} menggunakan \textit{bi encoder} untuk memetakan kueri dan dokumen ke dalam ruang vektor yang sama secara independen, yang memungkinkan pencarian berbasis \textit{nearest neighbor search} atau \textit{Maximum Inner Product Search (MIPS)} yang sangat efisien pada skala miliaran dokumen.

Meskipun demikian, pendekatan \textit{dense retrieval} murni terkadang kehilangan detail leksikal spesifik, seperti nama entitas atau nomor hukum yang presisi. Untuk mengatasi hal ini, model hibrida dan arsitektur \textit{late interaction} seperti \textit{ColBERT} dikembangkan, yang mempertahankan representasi tingkat token hingga tahap pencocokan akhir, memberikan keseimbangan antara pemahaman semantik mendalam dan presisi pencocokan istilah \autocite{wang2022improving}. 

Selain itu, integrasi model bahasa besar \textit{Large Language Models} dalam kerangka kerja \textit{IR}, misalnya untuk ekspansi kueri \textit{query expansion} atau augmentasi data pelatihan \textit{data augmentation}, terbukti meningkatkan kinerja model \textit{retrieval} secara signifikan. \textcite{silva2024improving} menunjukkan bahwa penggunaan \textit{LLM} untuk menghasilkan kueri sintetis dari deskripsi dataset \textit{query generation} dan menggunakannya untuk melatih ulang model \textit{dense retriever} dapat meningkatkan metrik NDCG at five hingga enam puluh sembilan persen dibandingkan model pra latih standar, khususnya dalam skenario di mana data latih berlabel sangat terbatas seperti \textit{zero shot} atau \textit{few shot settings}.

\subsection{Akuisisi Data Web dan \textit{Crawling} Terfokus}

Akuisisi data dari sumber web heterogen merupakan tahap fundamental dalam pembangunan korpus legislatif digital. Metode standar yang digunakan adalah \textit{web crawling} atau \textit{web scraping}, yaitu proses otomatisasi pengambilan konten dari \textit{World Wide Web} melalui agen perangkat lunak (\textit{bots}) yang menavigasi struktur \textit{hyperlink} secara sistematis. 

Dalam konteks pengumpulan data spesifik domain yang tersebar di berbagai subdomain, pendekatan \textit{General Purpose Crawling} sering kali tidak efisien karena cenderung mengunduh sejumlah besar halaman yang tidak relevan. Sebagai solusi, teknik \textit{Focused Crawling} dikembangkan untuk memprioritaskan pengunduhan halaman yang memiliki relevansi topik tinggi dengan tujuan pencarian. 

\textcite{dhanith2024weakly} mengusulkan mekanisme \textit{weakly supervised learning} berbasis \textit{GRU} (\textit{Gated Recurrent Unit}) untuk meningkatkan modul komputasi relevansi pada \textit{focused crawler}, yang memungkinkan agen untuk menyaring konten tidak penting dan hanya mengekstrak bagian halaman yang mengandung informasi target, bahkan dalam lingkungan web dinamis dengan label data yang minim.

Tantangan utama dalam akuisisi data web modern adalah penanganan konten yang tidak terstruktur dan variasi format penyajian. 

\textcite{nguyen2025developing} mendemonstrasikan kerangka kerja otomatis yang menggabungkan teknik \textit{crawling} dengan pemrosesan bahasa alami (\textit{Natural Language Processing} \textit{NLP}) untuk mengkategorikan informasi dari situs web secara \textit{real time}. Pendekatan ini tidak hanya mengambil teks mentah (\textit{raw text}), tetapi juga memanfaatkan struktur \textit{DOM} (\textit{Document Object Model}) dan \textit{metadata} \textit{HTML} untuk mempertahankan konteks hierarkis dokumen. Selain itu, untuk menangani skala data yang besar, strategi penjadwalan terdistribusi (\textit{distributed scheduling strategy}) menjadi krusial. 

\textcite{wang2023paths} memperkenalkan strategi penjadwalan berbasis \textit{rasa lapar} (\textit{hunger based strategy}) untuk \textit{crawler} terdistribusi, yang secara dinamis menyeimbangkan beban kerja antar node pengunduh untuk memaksimalkan \textit{throughput} dan cakupan halaman (\textit{coverage}) tanpa membebani server target secara berlebihan. Penerapan teknik teknik ini menjamin bahwa proses akuisisi dokumen legislatif dapat berjalan secara efisien, terukur (\textit{scalable}), dan mampu beradaptasi dengan perubahan struktur situs web sumber.

\subsubsection{\textit{Focused Crawling}}

\textit{Focused Crawling} merupakan paradigma dalam akuisisi data web yang dirancang untuk meningkatkan efisiensi pengumpulan halaman dengan membatasi eksplorasi hanya pada halaman yang relevan dengan topik atau domain tertentu. Berbeda dengan \textit{general purpose web crawler} yang melakukan penjelajahan lebar (\textit{breadth first search}) tanpa diskriminasi konten, \textit{focused crawler} dilengkapi dengan mekanisme penilaian relevansi (\textit{relevance scoring}) yang memprediksi apakah sebuah \textit{URL} kandidat mengarah ke konten yang sesuai dengan tujuan \textit{crawling}. Secara algoritmik, \textit{focused crawler} beroperasi dengan memelihara antrian prioritas (\textit{priority queue}) yang mengurutkan \textit{URL} berdasarkan skor relevansi yang dihitung oleh fungsi klasifikasi, yang dapat berbasis pada analisis konten halaman induk, struktur \textit{URL}, atau teks jangkar (\textit{anchor text}). 

\textcite{dhanith2024weakly} menjelaskan bahwa pendekatan pembelajaran terawasi lemah (\textit{weakly supervised learning}) dapat diterapkan untuk melatih modul prediktor relevansi menggunakan \textit{dataset} berlabel tidak sempurna, di mana model \textit{Gated Recurrent Unit} (\textit{GRU}) digunakan untuk menangkap pola sekuensial dalam representasi fitur halaman web guna meningkatkan akurasi deteksi konten relevan.

Untuk mengatasi keterbatasan pendekatan berbasis heuristik statis, teknik pembelajaran penguatan (\textit{Reinforcement Learning} \textit{RL}) telah diintegrasikan ke dalam strategi \textit{crawling} untuk memungkinkan agen belajar secara adaptif dari interaksi dengan lingkungan web. Dalam kerangka kerja ini, \textit{crawler} dimodelkan sebagai agen \textit{RL} yang menerima status (\textit{state}) berupa representasi halaman saat ini, memilih aksi berupa \textit{URL} berikutnya yang akan dikunjungi, dan menerima \textit{reward} berdasarkan relevansi halaman yang berhasil diunduh. 

\textcite{nguyen2025developing} menunjukkan bahwa kombinasi teknik \textit{crawling} dengan klasifikasi teks berbasis \textit{NLP} secara \textit{real time} memungkinkan kategorisasi konten segera setelah diunduh, sehingga umpan balik relevansi dapat digunakan untuk menyesuaikan strategi eksplorasi secara dinamis. Selain itu, metode optimasi bobot kata kunci (\textit{keyword weight optimization}) telah dikembangkan untuk meningkatkan presisi \textit{focused crawler}, di mana bobot \textit{term} dalam vektor \textit{query} diperbarui secara iteratif menggunakan strategi gradien berdasarkan \textit{feedback} dari halaman yang telah diunduh, memastikan bahwa \textit{crawler} semakin terfokus pada \textit{subdomain} yang paling relevan seiring berjalannya waktu \autocite{abbasi2021keyword}.

\subsection{Pra pemrosesan Dokumen dan Analisis Tata Letak (\textit{Layout Analysis})}

Kualitas data yang diekstrak dari korpus legislatif sangat bergantung pada efektivitas tahap pra pemrosesan, terutama mengingat heterogenitas format dokumen yang mencakup \textit{PDF} berbasis teks (\textit{born digital}) dan hasil pindaian (\textit{scanned images}) dengan tingkat degradasi visual yang bervariasi. Strategi \textit{OCR} selektif diterapkan untuk menyeimbangkan akurasi dan efisiensi komputasi. Untuk dokumen pindaian, penerapan teknik \textit{binarization} adaptif dan koreksi kemiringan (\textit{skew correction}) terbukti krusial sebelum proses pengenalan karakter dilakukan. \textcite{ignasius2023image} menunjukkan bahwa kombinasi \textit{denoising} dan \textit{skew correction} sebagai langkah pra \textit{OCR} dapat secara signifikan menurunkan \textit{Character Error Rate} (\textit{CER}) pada dokumen berlatar belakang kompleks. 

Lebih lanjut, untuk menangani artefak visual pada risalah lama, algoritma penghapusan derau (\textit{noise removal}) berbasis optimalisasi gabungan (\textit{joint optimization}) digunakan untuk memisahkan karakter tulisan dari noda latar belakang tanpa merusak integritas morfologi huruf. Pendekatan ini berdampak langsung pada peningkatan akurasi pengenalan mesin yang beroperasi di atas lapisan teks hasil ekstraksi citra.

Tantangan utama dalam memproses dokumen legislatif adalah kompleksitas tata letak (\textit{layout}) yang sering kali memuat elemen non linier seperti tabel, \textit{header}, \textit{footer}, dan kolom ganda. Metode analisis tata letak dokumen (\textit{Document Layout Analysis} \textit{DLA}) modern telah beralih dari pendekatan berbasis aturan heuristik menuju arsitektur \textit{deep learning}. \textcite{ignasius2023image} memperkenalkan kerangka kerja berbasis \textit{YOLO} (\textit{You Only Look Once}) yang dimodifikasi dengan mekanisme konvolusi dinamis untuk mendeteksi elemen tata letak multi skala secara simultan. Pendekatan ini memungkinkan sistem untuk membedakan antara blok teks naratif, tabel data presensi, dan judul sesi dengan presisi tinggi (\textit{mAP} di atas 90 persen), bahkan pada dokumen dengan struktur non Manhattan yang rumit. 

Selain itu, penggunaan model transformer multimodal seperti \textit{LayoutLMv3} memungkinkan integrasi fitur visual dan tekstual untuk memahami semantik spasial dokumen. \textcite{ignasius2023image} mengusulkan kerangka kerja fusi dinamis (\textit{Dynamic Fusion Network}) yang secara efektif menggabungkan fitur visual global dan detail tekstual lokal. Pendekatan ini memastikan bahwa urutan pembacaan logis (\textit{logical reading order}) dapat direkonstruksi dengan benar meskipun elemen fisik dokumen tersebar secara acak. Kemampuan ini esensial untuk mengubah halaman risalah rapat yang statis menjadi aliran teks terstruktur yang siap diproses oleh komponen \textit{NLP} di tahap selanjutnya.

\subsubsection{\textit{Optical Character Recognition} (\textit{OCR})}

\textit{Optical Character Recognition} (\textit{OCR}) adalah teknologi komputasional yang mengonversi gambar dokumen yang mengandung teks, baik hasil pindaian (\textit{scanned}), foto, maupun tangkapan layar, menjadi data teks yang dapat diedit dan dicari secara mesin. Secara arsitektural, sistem \textit{OCR} modern terdiri dari beberapa tahapan pemrosesan, yaitu pra pemrosesan gambar (\textit{image preprocessing}), segmentasi karakter (\textit{character segmentation}), pengenalan karakter (\textit{character recognition}), dan pasca pemrosesan (\textit{post processing}) untuk koreksi kesalahan. 

Pra pemrosesan mencakup operasi seperti \textit{binarization} (konversi gambar berwarna menjadi hitam putih), \textit{deskewing} (koreksi kemiringan dokumen), dan penghilangan derau (\textit{noise removal}) untuk meningkatkan kontras antara karakter dan latar belakang. \textcite{wang2023paths} menjelaskan bahwa arsitektur berbasis \textit{deep learning}, khususnya model \textit{Convolutional Recurrent Neural Networks} (\textit{CRNN}), telah mendominasi tugas pengenalan teks dalam citra (\textit{scene text recognition}) dan mengungguli metode tradisional berbasis fitur tangan (\textit{hand crafted features}) dengan margin akurasi yang signifikan. 

Meskipun \textit{OCR} telah mencapai akurasi tinggi pada teks cetak bersih, tantangan tetap ada pada dokumen dengan kualitas rendah, teks berlatar belakang kompleks, atau distorsi geometris. Untuk menangani kesalahan \textit{OCR} (\textit{OCR errors}), teknik pasca pemrosesan berbasis pemrosesan bahasa alami (\textit{Natural Language Processing} \textit{NLP}) telah dikembangkan. \textcite{ignasius2023image} mengusulkan algoritma koreksi pasca \textit{OCR} (\textit{OCR post correction}) yang memanfaatkan model \textit{sequence to sequence} (\textit{Seq2Seq}) untuk mendeteksi dan memperbaiki kesalahan pengenalan karakter secara kontekstual. 

Pendekatan ini mengintegrasikan pengetahuan linguistik, seperti model bahasa \textit{n gram} dan representasi \textit{BERT}, untuk memprediksi kata yang paling mungkin benar berdasarkan konteks kalimat, sehingga meningkatkan \textit{robustness} sistem terhadap gambar teks yang telah dimanipulasi atau mengandung derau \textit{adversarial}. Selain itu, untuk ekstraksi data tabular dari dokumen pindaian yang kompleks, \textcite{ignasius2023image} mendemonstrasikan kerangka kerja hibrida yang menggabungkan deteksi tabel berbasis \textit{DETR} (\textit{DEtection TRansformer}) dengan model pengenalan struktur tabel \textit{EDD} (\textit{Encoder Dual Decoder}). Kerangka ini mencapai skor \textit{Tree Edit Distance based Similarity} (\textit{TEDS}) sebesar 0{,}699, yang menunjukkan kemampuan tinggi dalam merekonstruksi struktur dan konten tabel secara simultan dari gambar beresolusi rendah.

\subsection{\textit{Event Log}}

\textit{Event log} adalah struktur data fundamental dalam sistem informasi proses (\textit{process aware information systems}) yang merekam jejak aktivitas sebagai sekuens peristiwa (\textit{event}) yang terjadi selama eksekusi proses bisnis atau sistem komputasional. Secara formal, sebuah \textit{event log} $L$ didefinisikan sebagai koleksi dari \textit{traces} atau \textit{cases}, di mana setiap \textit{case} merepresentasikan satu instansi eksekusi proses dan terdiri dari sekuens peristiwa yang terurut secara kronologis. Setiap peristiwa minimal memiliki atribut \textit{activity name} (jenis aktivitas yang dilakukan), \textit{timestamp} (waktu kejadian), dan secara opsional \textit{resource} (pelaku atau sistem yang melakukan aktivitas tersebut) serta \textit{data attributes} tambahan yang merekam konteks spesifik. \textcite{landauer2023deep} menjelaskan bahwa struktur ini memungkinkan analisis retrospektif terhadap perilaku sistem, deteksi anomali, dan rekonstruksi alur kerja (\textit{workflow}) yang aktual. 

Dalam praktiknya, \textit{event log} diperoleh dari sistem \textit{monitoring} seperti basis data transaksional, \textit{server log}, atau aplikasi \textit{enterprise} yang secara otomatis mencatat setiap interaksi pengguna dan operasi sistem. Tantangan utama dalam pengelolaan \textit{event log} adalah ketidaksempurnaan data (\textit{log imperfection}), yang mencakup \textit{missing events} (peristiwa yang tidak tercatat), \textit{incorrect timestamps}, dan \textit{interleaved logs} dari proses paralel yang tidak memiliki \textit{identifier} sesi yang jelas. 

Untuk mengatasi hal ini, teknik pra pemrosesan seperti \textit{log parsing} menjadi esensial. \textcite{alzu2024cyberattack} mengusulkan pendekatan klasifikasi berbasis semantik menggunakan \textit{BERT} untuk menganalisis pesan log yang tidak terstruktur dan mengekstrak pola semantik dari teks deskriptif log. Pendekatan ini memungkinkan sistem untuk secara otomatis mengkategorikan ribuan baris log ke dalam tipe peristiwa yang konsisten, yang kemudian dapat digunakan untuk analisis lanjutan seperti deteksi intrusi atau audit keamanan. 

Selain itu, teknik \textit{process mining}, yaitu ekstraksi model proses dari \textit{event log}, telah menjadi domain penelitian aktif. \textcite{bantay2023frequent} mendemonstrasikan penggunaan \textit{frequent pattern mining} untuk mempartisi log besar menjadi \textit{sub log} yang merepresentasikan proses paralel yang berbeda. Pendekatan ini memungkinkan eksplorasi dan analisis proses yang lebih granular dengan kompleksitas komputasi yang lebih rendah. Kemampuan untuk mentransformasi log mentah menjadi representasi terstruktur ini menjadikan \textit{event log} sebagai fondasi bagi sistem audit otomatis dan optimisasi proses operasional.

\subsection{Penautan Semantik (\textit{Semantic Linking})}

Penautan semantik (\textit{Semantic Linking}) adalah proses komputasional untuk menghubungkan entitas tekstual atau fragmen data yang terpisah ke dalam satu referensi makna yang kohesif, sering kali melintasi batas dokumen atau \textit{dataset} yang berbeda. Dalam arsitektur sistem informasi, tugas ini sering dioperasionalkan sebagai \textit{Entity Linking} (\textit{EL}) atau \textit{Semantic Matching}, di mana tujuannya adalah memetakan penyebutan entitas (\textit{entity mention}) dalam teks sumber ke entitas target dalam basis pengetahuan (\textit{knowledge base}) atau dokumen referensi lain. \textcite{ruas2022nilinker} menjelaskan bahwa tantangan utama dalam \textit{EL} adalah ambiguitas nama dan ketiadaan entitas dalam basis data (\textit{NIL entities}). Untuk mengatasi hal ini, pendekatan berbasis atensi (\textit{attention mechanism}) digunakan untuk membobot fitur kontekstual di sekitar penyebutan entitas, sehingga model dapat membedakan makna entitas berdasarkan konteks kalimatnya dan memprediksi tautan yang benar bahkan untuk entitas yang jarang muncul (\textit{long tail entities}). 

Selain pencocokan entitas eksplisit, penautan semantik modern juga memanfaatkan representasi vektor multidimensi untuk mengukur kesamaan makna antara dua segmen teks yang mungkin tidak memiliki tumpang tindih kata kunci (\textit{vocabulary mismatch}). \textcite{huang2022semantic} memperkenalkan kerangka kerja pencocokan saraf (\textit{neural matching network}) yang menggabungkan fitur leksikal (\textit{TF IDF}), fitur kontekstual statis (\textit{Word2Vec}), dan fitur dinamis (\textit{ELMo} dan \textit{BERT}) untuk membangun representasi semantik yang komprehensif. Dengan menghitung matriks kesamaan kosinus pada ruang vektor gabungan ini, sistem dapat mengidentifikasi hubungan semantik laten antara dokumen dengan akurasi yang jauh lebih tinggi dibandingkan metode pencocokan kata kunci tradisional. 

Lebih lanjut, untuk menangkap hubungan implisit yang kompleks, \textcite{li2025extracting} mengusulkan ekstraksi Jaringan Tautan Semantik Kata (\textit{Word Semantic Link Network} \textit{W SLN}) yang dilengkapi dengan aturan penalaran (\textit{reasoning rules}). Metode ini memungkinkan sistem untuk menurunkan hubungan baru yang tidak tertulis secara eksplisit dalam teks, serta memfasilitasi penemuan pola keterkaitan yang lebih dalam antara konsep yang tersebar dalam korpus data yang besar.

\section{Analytic Hierarchy Process (AHP)}

\textit{Analytic Hierarchy Process} (AHP) adalah metode pengambilan keputusan multikriteria (\textit{Multi-Criteria Decision Making} / MCDM) yang dikembangkan oleh Thomas L. Saaty pada tahun 1970-an untuk menangani masalah keputusan yang kompleks dengan cara menguraikan masalah menjadi struktur hierarkis dan melakukan perbandingan berpasangan antar elemen \autocite{leal2020ahp, kumar2023analytical}. Metode ini telah digunakan secara luas dalam berbagai bidang seperti pemilihan teknologi, manajemen risiko, dan perencanaan strategis karena kemampuannya mengubah penilaian kualitatif menjadi nilai kuantitatif yang terukur\autocite{kriswardhana2025analytic}.

AHP bekerja melalui tiga tahapan utama: (1) pembentukan hierarki masalah yang terdiri dari tujuan, kriteria, dan alternatif; (2) penentuan bobot prioritas melalui perbandingan berpasangan menggunakan skala numerik; dan (3) agregasi bobot untuk menentukan peringkat alternatif terbaik. Keunggulan AHP terletak pada kemampuannya menangani faktor-faktor subjektif sekaligus menjaga konsistensi logis melalui perhitungan rasio konsistensi (\textit{Consistency Ratio}, CR)\autocite{guillen2023lessons}.

\subsection{Skala Saaty dalam Perbandingan Berpasangan}
Inti dari metode AHP adalah penggunaan \textit{Saaty's Fundamental Scale} untuk melakukan perbandingan berpasangan antar kriteria atau alternatif. Skala ini menggunakan nilai numerik dari 1 hingga 9, di mana nilai 1 menunjukkan dua elemen memiliki kepentingan yang sama, sedangkan nilai 9 menunjukkan satu elemen jauh lebih penting dibandingkan elemen lainnya \autocite{leal2020ahp}. Tabel berikut merangkum interpretasi skala Saaty:

\begin{table}[H]
    \centering
    \caption{Skala Saaty untuk Perbandingan Berpasangan}
    \label{tab:skala-saaty}
    \renewcommand{\arraystretch}{1.3}
    \begin{tabular}{|c|l|}
        \hline
        \textbf{Nilai} & \textbf{Interpretasi} \\
        \hline
        1 & Sama pentingnya (\textit{Equal importance}) \\
        3 & Sedikit lebih penting (\textit{Moderate importance}) \\
        5 & Lebih penting (\textit{Strong importance}) \\
        7 & Sangat lebih penting (\textit{Very strong importance}) \\
        9 & Mutlak lebih penting (\textit{Extreme importance}) \\
        2, 4, 6, 8 & Nilai antara dua penilaian berurutan \\
        \hline
    \end{tabular}
\end{table}

Setelah matriks perbandingan berpasangan disusun, AHP menghitung vektor prioritas (\textit{priority vector}) menggunakan metode \textit{eigenvalue} atau normalisasi rata-rata geometrik. Konsistensi penilaian dievaluasi melalui \textit{Consistency Index} (CI) dan \textit{Consistency Ratio} (CR), di mana nilai CR $< 0.1$ dianggap dapat diterima\autocite{frish2025enhancing}. Dalam konteks penelitian ini, AHP digunakan untuk memilih arsitektur solusi terbaik bagi sistem \textit{Legislative Activity Tracker} dengan mempertimbangkan kriteria seperti efektivitas, skalabilitas, dampak transparansi, kemudahan implementasi, dan kualitas data.

% Gambar umumnya tidak jelas atau kabur jika gambar tersebut:
% \begin{enumerate}[a.]
%   \item diperoleh dari hasil cropping pada suatu halaman buku atau situs web;
%   \item hasil pembesaran gambar yang gambar aslinya sebenarnya berukuran kecil; atau
%   \item disimpan dalam resolusi kecil
% \end{enumerate}
% Ketidakjelasan gambar ini dapat dilihat pada garis-garis diagram yang tidak tegas dan tulisan-tulisan dalam gambar yang tampak kabur dan kurang jelas terbaca.

% Untuk mendapatkan gambar yang tidak kabur (\textit{blur}), langkah-langkah berikut dapat digunakan:
% \begin{enumerate}[(a)]
% \item Gambar yang didapat di suatu pustaka atau referensi sebaiknya digambar ulang, misalnya menggunakan PowerPoint, Canva, Figma, draw.io, atau yang lainnya.
% \item Jika diagram atau ilustrasi digambar menggunakan draw.io, saat gambar disimpan ke format PNG atau JPG (\textit{export as}), lakukan \textit{zoom} ke minimal 300\% (\textit{the default value is} 100\%). 
% \item Jika diagram digambar dengan menggunakan PowerPoint, gambar dapat langsung di-\textit{copy-paste} ke Word.
% \end{enumerate}

% \subsection{Tabel}
% Tabel ada dua jenis, yaitu tabel yang bisa termuat dalam satu halaman dan tabel yang sangat panjang sehingga tidak muat dalam satu halaman.
% \subsubsection{Tabel yang Muat dalam Satu Halaman}
% Contoh tabel dapat dilihat pada Tabel \ref{tbl:harga1} dan \ref{tbl:harga2}. Tabel dan judulnya dibuat rata kiri dan judul tabel diletakkan di atas tabel. Usahakan tabel dapat ditulis dalam satu halaman, tidak terpotong ke halaman berikutnya.

% \begin{table}[t] % pilihan opsi yang disarankan: t = top, b = bottom, h = here
%   \begin{tabular}{ | p{2cm} | p{2cm} | p{3cm} |}
% 	\hline
% 	Nama 	& Satuan 		& Harga \\
% 	\hline
% 	Buku 	& Exemplar	& 25000 \\
% 	Komputer	& Unit		& 2500000 \\
% 	Pensil	& Buah		& 118900 \\
% 	\hline
% 	\end{tabular}
% \caption{Tabel harga bahan pokok}
% \label{tbl:harga1}
% \end{table}



% \begin{table}[t] % pilihan opsi yang disarankan: t = top, b = bottom, h = here
% 	\begin{tabular}{ | l | c | r | }
% 	\hline
% 	Nama 	& Satuan 		& Harga \\
% 	\hline
% 	Buku 	& Exemplar	& 25000 \\
% 	Komputer	& Unit		& 2500000 \\
% 	Pensil	& Buah		& 118900 \\
% 	\hline
% 	\end{tabular}
% \caption{Tabel harga bahan sekunder}
% \label{tbl:harga2}
% \end{table}

% -- Example of importing table from external file --
% \subsubsection{Mengimpor Tabel dari Berkas Eksternal}

% Tabel \ref{tbl:harga3} diimpor dari berkas eksternal \textit{table/tabel1.tex} menggunakan perintah \textit{input}. 
% Dengan demikian, jika tabel tersebut perlu diubah, cukup mengubah pada berkas eksternal tersebut tanpa perlu mengubah pada berkas utama ini.

% \input table/tabel1.tex


% -- Example of long table --
% \subsubsection{Tabel yang Sangat Panjang}
% Jika tabel terlalu panjang sehingga tidak muat dalam satu halaman, gunakan paket 
% \textit{longtable} untuk membuat tabel yang dapat terpotong ke halaman berikutnya, 
% seperti pada Tabel \ref{tbl:longtable1}.

% \begin{longtable}{@{\extracolsep{\fill}} l c r r}
% \caption{Comprehensive Data Table Example}\label{tbl:longtable1} \\
% \midrule
% \textbf{ID} & \textbf{Name} & \textbf{Score} & \textbf{Rank} \\
% \midrule
% \endfirsthead

% \caption[]{Comprehensive Data Table Example (lanjutan)} \\
% \midrule
% \textbf{ID} & \textbf{Name} & \textbf{Score} & \textbf{Rank} \\
% \midrule
% \endhead

% \midrule
% \multicolumn{4}{r}{\textit{Bersambung ke halaman berikutnya}} \\
% %\bottomrule
% \endfoot

% \midrule
% \endlastfoot

% % === Table Data ===
% 1 & Alice Smith & 89 & 5 \\
% 2 & Bob Johnson & 93 & 3 \\
% 3 & Carol Davis & 95 & 2 \\
% 4 & Daniel Wilson & 88 & 6 \\
% 5 & Eve Thompson & 97 & 1 \\
% 6 & Frank Brown & 85 & 7 \\
% 7 & Grace Lee & 91 & 4 \\
% 8 & Henry Miller & 80 & 9 \\
% 9 & Irene Garcia & 83 & 8 \\
% 10 & Jack Robinson & 78 & 10 \\
% % Repeat with more rows to make it long
% 11 & Kevin Harris & 76 & 11 \\
% 12 & Laura Martin & 75 & 12 \\
% 13 & Michael Clark & 74 & 13 \\
% 14 & Natalie Lewis & 73 & 14 \\
% 15 & Olivia Walker & 72 & 15 \\
% 16 & Peter Hall & 71 & 16 \\
% 17 & Quinn Allen & 70 & 17 \\
% 18 & Rachel Young & 69 & 18 \\
% 19 & Samuel King & 68 & 19 \\
% 20 & Tina Wright & 67 & 20 \\
% 21 & Uma Scott & 66 & 21 \\
% 22 & Victor Green & 65 & 22 \\
% 23 & Wendy Adams & 64 & 23 \\
% 24 & Xavier Nelson & 63 & 24 \\
% 25 & Yolanda Carter & 62 & 25 \\
% 26 & Zachary Perez & 61 & 26 \\
% 27 & Amelia Baker & 60 & 27 \\
% 28 & Benjamin Rivera & 59 & 28 \\
% 29 & Charlotte Rogers & 58 & 29 \\
% 30 & David Murphy & 57 & 30 \\
% 31 & Ethan Cooper & 56 & 31 \\
% 32 & Fiona Reed & 55 & 32 \\
% 33 & George Bailey & 54 & 33 \\
% 34 & Hannah Cox & 53 & 34 \\
% 35 & Isaac Howard & 52 & 35 \\
% 36 & Julia Ward & 51 & 36 \\
% 37 & Kyle Flores & 50 & 37 \\
% 38 & Lily Bell & 49 & 38 \\
% 39 & Mason Sanders & 48 & 39 \\
% 40 & Nora Patterson & 47 & 40 \\
% 41 & Owen Ramirez & 46 & 41 \\
% 42 & Penelope Torres & 45 & 42 \\
% 43 & Quentin Foster & 44 & 43 \\
% 44 & Rebecca Gonzales & 43 & 44 \\
% 45 & Sebastian Bryant & 42 & 45 \\
% 46 & Taylor Alexander & 41 & 46 \\
% 47 & Ursula Russell & 40 & 47 \\
% 48 & Vincent Griffin & 39 & 48 \\
% 49 & William Diaz & 38 & 49 \\
% 50 & Zoe Simmons & 37 & 50 \\
% % (You can easily extend this list to hundreds of rows)
% \end{longtable}

% \subsubsection{Beberapa Contoh Penulisan Rumus atau Persamaan Matematika Menggunakan LaTeX Termasuk Penomorannya}
% Contoh rumus matematika dapat ditulis seperti pada Persamaan \ref{eq:contoh1} di bawah ini. 
% Penomoran persamaan diletakkan di sebelah kanan, dan rumus ditulis dalam mode \textit{display math}.
% \begin{equation}
% E = mc^2
% \label{eq:contoh1}
% \end{equation}

% Contoh lain penulisan rumus matematika yang lebih kompleks dapat ditulis seperti pada Persamaan \ref{eq:rumus2}.

% \begin{align}
% f(x) &= ax^2 + bx + c \\
% f'(x) &= \frac{d}{dx}(ax^2 + bx + c) \notag \\ % tidak menampilkan nomor pada baris ini
%       &= 2ax + b \label{eq:rumus2}
% \end{align}

% Jika rumus terlalu panjang untuk ditulis dalam satu baris, gunakan lingkungan \textit{multline} seperti pada Persamaan \ref{eq:rumus3} di bawah ini.
% \begin{multline} 
% y = a_0 + a_1x + a_2x^2 + a_3x^3 + a_4x^4 + a_5x^5 + a_6x^6 + a_7x^7 \\
%     + a_8x^8 + a_9x^9 + a_{10}x^{10} \label{eq:rumus3}
% \end{multline}

% Jika ada penurunan rumus yang terdiri dari beberapa baris, namun tidak memerlukan penomoran pada setiap baris, gunakan lingkungan \textit{align*}, misalnya:

% \begin{align*} 
% S &= \sum_{i=1}^{n} i^2 \\
%   &= 1^2 + 2^2 + 3^2 + \cdots + n^2 \\
%   &= \frac{n(n + 1)(2n + 1)}{6}
% \intertext{Contoh lainnya adalah rumus untuk mencari nilai rata-rata fungsi $f(x)$ pada interval $[p, q]$:}
% \bar{f} &= \frac{1}{q - p} \int_{p}^{q} f(x) \, dx \\
%         &= \frac{1}{q - p} \int_{p}^{q} (ax^2 + bx + c) \, dx \\
%         &= \frac{1}{q - p} \left[ \frac{a}{3}x^3 + \frac{b}{2}x^2 + cx \right]_p^q \\
%         &= \frac{a(q^3 - p^3)}{3(q - p)} + \frac{b(q^2 - p^2)}{2(q - p)} + c \label{eq:rumus4}
% \end{align*}



% \subsection{Algoritma, Pseudocode, atau Kode}
% Contoh penulisan algoritma atau pseudocode dapat ditulis seperti pada Kode \ref{alg:contoh1} di bawah ini. 
% Gunakan paket \textit{listings} untuk menulis source code dalam bahasa pemrograman tertentu, seperti pada Kode \ref{lst:contoh1}. 


% % -- Example of pseudocode and source code listing --
% % -- Gunakan minipage agar listing tidak terpotong ke halaman berikutnya --
% \begin{minipage}{\textwidth} 
% \begin{lstlisting}[frame=lines, captionpos=t, caption={Contoh pseudocode}, label={alg:contoh1}]
% ALGORITHM HelloWorld
%    PRINT "Hello, World!"
% END ALGORITHM
% \end{lstlisting}
% \end{minipage}

% \begin{minipage}{\textwidth}
% \begin{lstlisting}[language=Python, frame=single, caption={Contoh source code Python}, captionpos=t, label={lst:contoh1}]
% def hello_world():
%     print("Hello, World!")       
% hello_world()
% \end{lstlisting}
% \end{minipage}


% \section{Beberapa Kesalahan Penulisan yang Sering Terjadi}
% \subsection{Penggunaan Kata "di mana" atau "dimana"}
% Banyak yang menuliskan kata "di mana" atau "dimana" sebagai pengganti kata "which" dalam bahasa Inggris. 
% Padahal, penggunaan kata "di mana" atau "dimana" tidak tepat dalam konteks tersebut. Demikian juga untuk kata serupa, misalnya "yang mana".
% Kata "di mana" atau "dimana" ini harus diganti dengan kata lain, seperti "dengan", "tempat", "yang", dan sebagainya tergantung kalimatnya.
% Penjelasan lengkap dapat dilihat pada \autocite{BPBI}.

% \subsection{Penggunaan Kata "sedangkan" dan "sehingga"}

% \begin{table}[t]
%   \begin{tabular}{|c|l|l|}
%   \hline
%   Kata & Salah & Benar \\ \hline
%   sedangkan & \begin{tabular}[c]{@{}c@{}}Sedangkan sistem lama masih\\ digunakan oleh banyak pengguna.\end{tabular} & \begin{tabular}[c]{@{}c@{}}Sistem lama masih digunakan\\ oleh banyak pengguna,\\ sedangkan sistem baru belum siap.\end{tabular} \\ \hline
%   sehingga & \begin{tabular}[c]{@{}c@{}}Sehingga sistem lama masih\\ digunakan oleh banyak pengguna.\end{tabular} & \begin{tabular}[c]{@{}c@{}}Sistem lama masih digunakan\\ oleh banyak pengguna sehingga\\ sistem baru belum siap.\end{tabular} \\ \hline
%   \end{tabular}
%   \caption{Contoh penggunaan kata "sedangkan" dan "sehingga"}
%   \label{tbl:sedangkan_sehingga}
% \end{table}

% Kata "sedangkan" dan "sehingga" adalah kata hubung atau konjungsi. 
% Konjungsi adalah kata atau ungkapan yang menghubungkan satuan bahasa 
% (kata, frasa, klausa, dan kalimat). 
% Konjungsi dapat dibagi menjadi konjungsi intrakalimat dan antarkalimat.  
% Kata "sedangkan" menghubungkan dua klausa yang bersifat kontrasif, 
% sedangkan "sehingga" menghubungkan dua klausa yang bersifat kausal. 
% Dalam ragam formal, kata hubung “sedangkan” dan “sehingga” hanya dapat digunakan 
% sebagai konjungsi intrakalimat sehingga kedua konjungsi itu \textbf{tidak dapat diletakkan pada awal kalimat}.
% Selain itu, penggunaan kata "sedangkan" harus didahului oleh koma (,), sedangkan kata "sehingga" tidak perlu didahului oleh koma (,).
% Contoh penggunaan yang benar dan salah dapat dilihat pada Tabel \ref{tbl:sedangkan_sehingga}.


% \subsection{Penggunaan Istilah yang Tidak Baku}
% Ada beberapa istilah yang sering digunakan dalam pembicaraan sehari-hari, tetapi tidak baku dalam penulisan ilmiah.
% Beberapa istilah tersebut antara lain:
% \begin{enumerate}
%   \item analisa $\rightarrow$ analisis
%   \item eksisting atau existing $\rightarrow$ yang ada atau saat ini
%   \item bisnis proses $\rightarrow$ proses bisnis
%   \item user $\rightarrow$ pengguna
%   \item system $\rightarrow$ sistem
%   \item database $\rightarrow$ basis data
%   \item aktifitas $\rightarrow$ aktivitas
%   \item efektifitas $\rightarrow$ efektivitas
%   \item sosial media $\rightarrow$ media sosial
% \end{enumerate}

% \subsection{Pemisah Desimal dan Ribuan}
% Tanda pemisah desimal dalam bahasa Indonesia adalah tanda koma, contoh:
% \begin{enumerate}
%   \item (Salah) Akurasi naik menjadi 50.6\% 
%   \item (Benar) Akurasi naik menjadi 50,6\% 
% \end{enumerate}

% \subsection{Daftar atau \textit{List}}
% Ada beberapa aturan penulisan daftar atau \textit{list} yang perlu diperhatikan, antara lain:
% \begin{enumerate}[a)]
% \item Jika memungkinkan, hindari penggunaan “bullet points” atau sejenisnya. Sebaiknya, gunakan angka (1, 2, 3, ...) atau huruf (a, b, c, …). Dengan demikian, pembaca dapat dengan mudah melihat jumlah \textit{item} atau \textit{list}. 
% \item Jika dalam daftar hanya ada satu item, tidak perlu menggunakan nomor urut.
% \item Penjelasan atau deskripsi suatu item sebaiknya menyatu dengan judul item tersebut, tidak berbeda halaman. Contoh yang salah: judul item ada di halaman 10, namun deskripsinya di halaman 11. Sebaiknya pindahkan judul tersebut ke halaman 11.
% \item Jika penjelasan atau deskripsi suatu item cukup panjang, misalnya lebih dari 1 halaman atau terdiri atas beberapa paragraf, sebaiknya setiap item tersebut dijadikan judul subbab, kecuali jika level subbab sudah mencapai level 4. 
% \end{enumerate}

% \subsection{Penggunaan Kata "masing-masing" dan "setiap"}
% Kata “masing-masing” digunakan di belakang kata yang diterangkan, misalnya 
% "Setiap proses menggunakan algoritma masing-masing". Kata “tiap-tiap” atau “setiap”
% ditempatkan di depan kata yang diterangkan, misalnya
% "Setiap proses menggunakan algoritma tertentu".
